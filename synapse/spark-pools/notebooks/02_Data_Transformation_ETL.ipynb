{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Transformation and ETL Pipeline\n",
        "## Azure Synapse Analytics Data Platform (ASADP)\n",
        "\n",
        "This notebook demonstrates advanced data transformation and ETL processes using Apache Spark in Azure Synapse Analytics.\n",
        "\n",
        "### Features:\n",
        "- **Medallion Architecture**: Bronze â†’ Silver â†’ Gold data layers\n",
        "- **Delta Lake Integration**: ACID transactions and time travel\n",
        "- **Data Quality Validation**: Comprehensive data quality checks\n",
        "- **Schema Evolution**: Automatic schema management\n",
        "- **Performance Optimization**: Partitioning and Z-ordering\n",
        "\n",
        "### Prerequisites:\n",
        "- Azure Synapse Spark Pool configured\n",
        "- Data Lake Storage Gen2 with sample data\n",
        "- Delta Lake libraries installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta.tables import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "\n",
        "# Initialize Spark session with Delta Lake support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ASADP-ETL-Pipeline\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set configuration for better performance\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Delta Lake version: {spark.sql('SELECT delta_version()').collect()[0][0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "STORAGE_ACCOUNT = \"your_storage_account_name\"\n",
        "CONTAINER_RAW = \"raw\"\n",
        "CONTAINER_PROCESSED = \"processed\"\n",
        "CONTAINER_CURATED = \"curated\"\n",
        "\n",
        "# Data Lake paths\n",
        "BASE_PATH = f\"abfss://{CONTAINER_RAW}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n",
        "BRONZE_PATH = f\"abfss://{CONTAINER_RAW}@{STORAGE_ACCOUNT}.dfs.core.windows.net/bronze\"\n",
        "SILVER_PATH = f\"abfss://{CONTAINER_PROCESSED}@{STORAGE_ACCOUNT}.dfs.core.windows.net/silver\"\n",
        "GOLD_PATH = f\"abfss://{CONTAINER_CURATED}@{STORAGE_ACCOUNT}.dfs.core.windows.net/gold\"\n",
        "\n",
        "print(f\"Bronze Layer: {BRONZE_PATH}\")\n",
        "print(f\"Silver Layer: {SILVER_PATH}\")\n",
        "print(f\"Gold Layer: {GOLD_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Bronze Layer - Raw Data Ingestion\n",
        "\n",
        "The Bronze layer contains raw, unprocessed data from various sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample sales data for demonstration\n",
        "def generate_sample_sales_data(num_records=10000):\n",
        "    \"\"\"\n",
        "    Generate sample sales data for ETL demonstration\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "    import random\n",
        "    \n",
        "    # Sample data generation\n",
        "    customers = [f\"CUST_{i:06d}\" for i in range(1, 1001)]\n",
        "    products = [f\"PROD_{i:04d}\" for i in range(1, 101)]\n",
        "    regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
        "    categories = [\"Electronics\", \"Clothing\", \"Home\", \"Sports\", \"Books\"]\n",
        "    \n",
        "    data = []\n",
        "    base_date = datetime(2024, 1, 1)\n",
        "    \n",
        "    for i in range(num_records):\n",
        "        record = {\n",
        "            \"transaction_id\": f\"TXN_{i+1:08d}\",\n",
        "            \"customer_id\": random.choice(customers),\n",
        "            \"product_id\": random.choice(products),\n",
        "            \"product_category\": random.choice(categories),\n",
        "            \"region\": random.choice(regions),\n",
        "            \"quantity\": random.randint(1, 10),\n",
        "            \"unit_price\": round(random.uniform(10.0, 500.0), 2),\n",
        "            \"discount_percent\": random.choice([0, 5, 10, 15, 20]),\n",
        "            \"transaction_date\": (base_date + timedelta(days=random.randint(0, 365))).strftime(\"%Y-%m-%d\"),\n",
        "            \"transaction_timestamp\": (base_date + timedelta(days=random.randint(0, 365), \n",
        "                                                          hours=random.randint(0, 23), \n",
        "                                                          minutes=random.randint(0, 59))).isoformat(),\n",
        "            \"sales_rep_id\": f\"REP_{random.randint(1, 50):03d}\",\n",
        "            \"channel\": random.choice([\"Online\", \"Store\", \"Phone\", \"Mobile App\"]),\n",
        "            \"payment_method\": random.choice([\"Credit Card\", \"Debit Card\", \"Cash\", \"PayPal\"])\n",
        "        }\n",
        "        data.append(record)\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Generate sample data\n",
        "print(\"Generating sample sales data...\")\n",
        "sample_data = generate_sample_sales_data(50000)\n",
        "\n",
        "# Create DataFrame\n",
        "sales_df = spark.createDataFrame(sample_data)\n",
        "\n",
        "print(f\"Generated {sales_df.count()} sample records\")\n",
        "sales_df.printSchema()\n",
        "sales_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to Bronze layer (Delta format)\n",
        "print(\"Writing data to Bronze layer...\")\n",
        "\n",
        "bronze_table_path = f\"{BRONZE_PATH}/sales_transactions\"\n",
        "\n",
        "# Add metadata columns for data lineage\n",
        "bronze_df = sales_df \\\n",
        "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
        "    .withColumn(\"source_system\", lit(\"sample_generator\")) \\\n",
        "    .withColumn(\"file_name\", lit(\"generated_data\")) \\\n",
        "    .withColumn(\"data_quality_score\", lit(1.0))\n",
        "\n",
        "# Write to Delta table with partitioning\n",
        "bronze_df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"transaction_date\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .save(bronze_table_path)\n",
        "\n",
        "print(f\"Bronze layer data written to: {bronze_table_path}\")\n",
        "\n",
        "# Create Delta table for SQL access\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS bronze_sales_transactions\n",
        "    USING DELTA\n",
        "    LOCATION '{bronze_table_path}'\n",
        "\"\"\")\n",
        "\n",
        "print(\"Bronze layer table created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Silver Layer - Data Cleansing and Validation\n",
        "\n",
        "The Silver layer contains cleaned, validated, and enriched data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Functions\n",
        "def validate_data_quality(df):\n",
        "    \"\"\"\n",
        "    Perform comprehensive data quality validation\n",
        "    \"\"\"\n",
        "    quality_checks = {}\n",
        "    \n",
        "    # Check for null values\n",
        "    null_counts = {}\n",
        "    for col in df.columns:\n",
        "        null_count = df.filter(col(col).isNull()).count()\n",
        "        null_counts[col] = null_count\n",
        "    \n",
        "    quality_checks['null_counts'] = null_counts\n",
        "    \n",
        "    # Check for duplicates\n",
        "    total_records = df.count()\n",
        "    distinct_records = df.distinct().count()\n",
        "    duplicate_count = total_records - distinct_records\n",
        "    \n",
        "    quality_checks['total_records'] = total_records\n",
        "    quality_checks['distinct_records'] = distinct_records\n",
        "    quality_checks['duplicate_count'] = duplicate_count\n",
        "    \n",
        "    # Check data ranges\n",
        "    quality_checks['quantity_stats'] = df.select(\n",
        "        min(\"quantity\").alias(\"min_quantity\"),\n",
        "        max(\"quantity\").alias(\"max_quantity\"),\n",
        "        avg(\"quantity\").alias(\"avg_quantity\")\n",
        "    ).collect()[0].asDict()\n",
        "    \n",
        "    quality_checks['price_stats'] = df.select(\n",
        "        min(\"unit_price\").alias(\"min_price\"),\n",
        "        max(\"unit_price\").alias(\"max_price\"),\n",
        "        avg(\"unit_price\").alias(\"avg_price\")\n",
        "    ).collect()[0].asDict()\n",
        "    \n",
        "    return quality_checks\n",
        "\n",
        "# Read from Bronze layer\n",
        "print(\"Reading data from Bronze layer...\")\n",
        "bronze_df = spark.read.format(\"delta\").load(bronze_table_path)\n",
        "\n",
        "# Perform data quality validation\n",
        "print(\"Performing data quality validation...\")\n",
        "quality_report = validate_data_quality(bronze_df)\n",
        "\n",
        "print(\"Data Quality Report:\")\n",
        "print(f\"Total Records: {quality_report['total_records']}\")\n",
        "print(f\"Distinct Records: {quality_report['distinct_records']}\")\n",
        "print(f\"Duplicate Count: {quality_report['duplicate_count']}\")\n",
        "print(f\"Quantity Stats: {quality_report['quantity_stats']}\")\n",
        "print(f\"Price Stats: {quality_report['price_stats']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Cleansing and Transformation\n",
        "print(\"Performing data cleansing and transformation...\")\n",
        "\n",
        "# Clean and transform data\n",
        "silver_df = bronze_df \\\n",
        "    .filter(col(\"quantity\") > 0) \\\n",
        "    .filter(col(\"unit_price\") > 0) \\\n",
        "    .filter(col(\"discount_percent\").between(0, 100)) \\\n",
        "    .withColumn(\"gross_amount\", col(\"quantity\") * col(\"unit_price\")) \\\n",
        "    .withColumn(\"discount_amount\", col(\"gross_amount\") * col(\"discount_percent\") / 100) \\\n",
        "    .withColumn(\"net_amount\", col(\"gross_amount\") - col(\"discount_amount\")) \\\n",
        "    .withColumn(\"transaction_year\", year(col(\"transaction_date\"))) \\\n",
        "    .withColumn(\"transaction_month\", month(col(\"transaction_date\"))) \\\n",
        "    .withColumn(\"transaction_quarter\", quarter(col(\"transaction_date\"))) \\\n",
        "    .withColumn(\"transaction_day_of_week\", dayofweek(col(\"transaction_date\"))) \\\n",
        "    .withColumn(\"is_weekend\", when(col(\"transaction_day_of_week\").isin([1, 7]), True).otherwise(False)) \\\n",
        "    .withColumn(\"price_category\", \n",
        "                when(col(\"unit_price\") < 50, \"Low\")\n",
        "                .when(col(\"unit_price\") < 200, \"Medium\")\n",
        "                .otherwise(\"High\")) \\\n",
        "    .withColumn(\"customer_segment\", \n",
        "                when(col(\"net_amount\") < 100, \"Bronze\")\n",
        "                .when(col(\"net_amount\") < 500, \"Silver\")\n",
        "                .when(col(\"net_amount\") < 1000, \"Gold\")\n",
        "                .otherwise(\"Platinum\")) \\\n",
        "    .withColumn(\"processed_timestamp\", current_timestamp()) \\\n",
        "    .withColumn(\"data_quality_flag\", lit(\"PASSED\"))\n",
        "\n",
        "# Add row hash for change detection\n",
        "silver_df = silver_df.withColumn(\n",
        "    \"row_hash\",\n",
        "    sha2(concat_ws(\"|\", \n",
        "                   col(\"transaction_id\"),\n",
        "                   col(\"customer_id\"),\n",
        "                   col(\"product_id\"),\n",
        "                   col(\"quantity\"),\n",
        "                   col(\"unit_price\")), 256)\n",
        ")\n",
        "\n",
        "print(f\"Silver layer records after cleansing: {silver_df.count()}\")\n",
        "silver_df.printSchema()\n",
        "silver_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write to Silver layer\n",
        "print(\"Writing data to Silver layer...\")\n",
        "\n",
        "silver_table_path = f\"{SILVER_PATH}/sales_transactions\"\n",
        "\n",
        "# Write to Delta table with optimization\n",
        "silver_df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"transaction_year\", \"transaction_month\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .option(\"optimizeWrite\", \"true\") \\\n",
        "    .save(silver_table_path)\n",
        "\n",
        "print(f\"Silver layer data written to: {silver_table_path}\")\n",
        "\n",
        "# Create Delta table for SQL access\n",
        "spark.sql(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS silver_sales_transactions\n",
        "    USING DELTA\n",
        "    LOCATION '{silver_table_path}'\n",
        "\"\"\")\n",
        "\n",
        "# Optimize Delta table\n",
        "spark.sql(f\"OPTIMIZE silver_sales_transactions ZORDER BY (customer_id, product_id)\")\n",
        "\n",
        "print(\"Silver layer table created and optimized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Gold Layer - Business-Ready Aggregations\n",
        "\n",
        "The Gold layer contains business-ready, aggregated data for analytics and reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read from Silver layer\n",
        "print(\"Reading data from Silver layer...\")\n",
        "silver_df = spark.read.format(\"delta\").load(silver_table_path)\n",
        "\n",
        "# Create business aggregations\n",
        "print(\"Creating business aggregations...\")\n",
        "\n",
        "# 1. Daily Sales Summary\n",
        "daily_sales = silver_df.groupBy(\n",
        "    \"transaction_date\",\n",
        "    \"transaction_year\",\n",
        "    \"transaction_month\",\n",
        "    \"transaction_quarter\"\n",
        ").agg(\n",
        "    count(\"transaction_id\").alias(\"total_transactions\"),\n",
        "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
        "    countDistinct(\"product_id\").alias(\"unique_products\"),\n",
        "    sum(\"quantity\").alias(\"total_quantity\"),\n",
        "    sum(\"gross_amount\").alias(\"total_gross_amount\"),\n",
        "    sum(\"discount_amount\").alias(\"total_discount_amount\"),\n",
        "    sum(\"net_amount\").alias(\"total_net_amount\"),\n",
        "    avg(\"net_amount\").alias(\"avg_transaction_value\"),\n",
        "    max(\"net_amount\").alias(\"max_transaction_value\"),\n",
        "    min(\"net_amount\").alias(\"min_transaction_value\")\n",
        ").withColumn(\"aggregation_level\", lit(\"daily\")) \\\n",
        " .withColumn(\"created_timestamp\", current_timestamp())\n",
        "\n",
        "print(f\"Daily sales aggregations: {daily_sales.count()} records\")\n",
        "daily_sales.show(5)\n",
        "\n",
        "# 2. Product Performance Summary\n",
        "product_performance = silver_df.groupBy(\n",
        "    \"product_id\",\n",
        "    \"product_category\",\n",
        "    \"transaction_year\",\n",
        "    \"transaction_month\"\n",
        ").agg(\n",
        "    count(\"transaction_id\").alias(\"total_transactions\"),\n",
        "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
        "    sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
        "    sum(\"net_amount\").alias(\"total_revenue\"),\n",
        "    avg(\"net_amount\").alias(\"avg_transaction_value\"),\n",
        "    avg(\"unit_price\").alias(\"avg_unit_price\"),\n",
        "    avg(\"discount_percent\").alias(\"avg_discount_percent\")\n",
        ").withColumn(\"aggregation_level\", lit(\"product_monthly\")) \\\n",
        " .withColumn(\"created_timestamp\", current_timestamp())\n",
        "\n",
        "print(f\"Product performance aggregations: {product_performance.count()} records\")\n",
        "product_performance.show(5)\n",
        "\n",
        "# 3. Customer Segmentation Summary\n",
        "customer_segmentation = silver_df.groupBy(\n",
        "    \"customer_id\",\n",
        "    \"customer_segment\",\n",
        "    \"transaction_year\"\n",
        ").agg(\n",
        "    count(\"transaction_id\").alias(\"total_transactions\"),\n",
        "    countDistinct(\"product_id\").alias(\"unique_products_purchased\"),\n",
        "    sum(\"quantity\").alias(\"total_quantity_purchased\"),\n",
        "    sum(\"net_amount\").alias(\"total_spent\"),\n",
        "    avg(\"net_amount\").alias(\"avg_transaction_value\"),\n",
        "    max(\"transaction_date\").alias(\"last_purchase_date\"),\n",
        "    min(\"transaction_date\").alias(\"first_purchase_date\")\n",
        ").withColumn(\"aggregation_level\", lit(\"customer_yearly\")) \\\n",
        " .withColumn(\"created_timestamp\", current_timestamp())\n",
        "\n",
        "print(f\"Customer segmentation aggregations: {customer_segmentation.count()} records\")\n",
        "customer_segmentation.show(5)\n",
        "\n",
        "# 4. Regional Performance Summary\n",
        "regional_performance = silver_df.groupBy(\n",
        "    \"region\",\n",
        "    \"transaction_year\",\n",
        "    \"transaction_quarter\"\n",
        ").agg(\n",
        "    count(\"transaction_id\").alias(\"total_transactions\"),\n",
        "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
        "    countDistinct(\"product_id\").alias(\"unique_products\"),\n",
        "    sum(\"net_amount\").alias(\"total_revenue\"),\n",
        "    avg(\"net_amount\").alias(\"avg_transaction_value\")\n",
        ").withColumn(\"aggregation_level\", lit(\"regional_quarterly\")) \\\n",
        " .withColumn(\"created_timestamp\", current_timestamp())\n",
        "\n",
        "print(f\"Regional performance aggregations: {regional_performance.count()} records\")\n",
        "regional_performance.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write Gold layer aggregations\n",
        "print(\"Writing aggregations to Gold layer...\")\n",
        "\n",
        "# Write daily sales summary\n",
        "daily_sales_path = f\"{GOLD_PATH}/daily_sales_summary\"\n",
        "daily_sales.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"transaction_year\", \"transaction_month\") \\\n",
        "    .option(\"optimizeWrite\", \"true\") \\\n",
        "    .save(daily_sales_path)\n",
        "\n",
        "# Write product performance\n",
        "product_performance_path = f\"{GOLD_PATH}/product_performance\"\n",
        "product_performance.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"product_category\", \"transaction_year\") \\\n",
        "    .option(\"optimizeWrite\", \"true\") \\\n",
        "    .save(product_performance_path)\n",
        "\n",
        "# Write customer segmentation\n",
        "customer_segmentation_path = f\"{GOLD_PATH}/customer_segmentation\"\n",
        "customer_segmentation.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"customer_segment\", \"transaction_year\") \\\n",
        "    .option(\"optimizeWrite\", \"true\") \\\n",
        "    .save(customer_segmentation_path)\n",
        "\n",
        "# Write regional performance\n",
        "regional_performance_path = f\"{GOLD_PATH}/regional_performance\"\n",
        "regional_performance.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"region\", \"transaction_year\") \\\n",
        "    .option(\"optimizeWrite\", \"true\") \\\n",
        "    .save(regional_performance_path)\n",
        "\n",
        "print(\"Gold layer aggregations written successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Gold layer tables for SQL access\n",
        "print(\"Creating Gold layer tables...\")\n",
        "\n",
        "# Create tables\n",
        "tables_config = [\n",
        "    (\"gold_daily_sales_summary\", daily_sales_path),\n",
        "    (\"gold_product_performance\", product_performance_path),\n",
        "    (\"gold_customer_segmentation\", customer_segmentation_path),\n",
        "    (\"gold_regional_performance\", regional_performance_path)\n",
        "]\n",
        "\n",
        "for table_name, table_path in tables_config:\n",
        "    spark.sql(f\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS {table_name}\n",
        "        USING DELTA\n",
        "        LOCATION '{table_path}'\n",
        "    \"\"\")\n",
        "    \n",
        "    # Optimize tables\n",
        "    spark.sql(f\"OPTIMIZE {table_name}\")\n",
        "    \n",
        "    print(f\"Created and optimized table: {table_name}\")\n",
        "\n",
        "print(\"All Gold layer tables created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Quality Monitoring and Alerting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Monitoring\n",
        "def create_data_quality_report():\n",
        "    \"\"\"\n",
        "    Create comprehensive data quality report\n",
        "    \"\"\"\n",
        "    report = {}\n",
        "    \n",
        "    # Bronze layer quality\n",
        "    bronze_df = spark.read.format(\"delta\").load(bronze_table_path)\n",
        "    report['bronze'] = {\n",
        "        'record_count': bronze_df.count(),\n",
        "        'null_transaction_ids': bronze_df.filter(col(\"transaction_id\").isNull()).count(),\n",
        "        'duplicate_transaction_ids': bronze_df.count() - bronze_df.select(\"transaction_id\").distinct().count(),\n",
        "        'invalid_quantities': bronze_df.filter(col(\"quantity\") <= 0).count(),\n",
        "        'invalid_prices': bronze_df.filter(col(\"unit_price\") <= 0).count()\n",
        "    }\n",
        "    \n",
        "    # Silver layer quality\n",
        "    silver_df = spark.read.format(\"delta\").load(silver_table_path)\n",
        "    report['silver'] = {\n",
        "        'record_count': silver_df.count(),\n",
        "        'data_quality_passed': silver_df.filter(col(\"data_quality_flag\") == \"PASSED\").count(),\n",
        "        'avg_transaction_value': silver_df.agg(avg(\"net_amount\")).collect()[0][0],\n",
        "        'total_revenue': silver_df.agg(sum(\"net_amount\")).collect()[0][0]\n",
        "    }\n",
        "    \n",
        "    # Gold layer quality\n",
        "    daily_sales_df = spark.read.format(\"delta\").load(daily_sales_path)\n",
        "    report['gold'] = {\n",
        "        'daily_summaries': daily_sales_df.count(),\n",
        "        'date_range': {\n",
        "            'min_date': daily_sales_df.agg(min(\"transaction_date\")).collect()[0][0],\n",
        "            'max_date': daily_sales_df.agg(max(\"transaction_date\")).collect()[0][0]\n",
        "        },\n",
        "        'total_aggregated_revenue': daily_sales_df.agg(sum(\"total_net_amount\")).collect()[0][0]\n",
        "    }\n",
        "    \n",
        "    return report\n",
        "\n",
        "# Generate quality report\n",
        "print(\"Generating data quality report...\")\n",
        "quality_report = create_data_quality_report()\n",
        "\n",
        "print(\"\\n=== DATA QUALITY REPORT ===\")\n",
        "print(f\"Bronze Layer:\")\n",
        "print(f\"  - Records: {quality_report['bronze']['record_count']:,}\")\n",
        "print(f\"  - Null Transaction IDs: {quality_report['bronze']['null_transaction_ids']}\")\n",
        "print(f\"  - Duplicate Transaction IDs: {quality_report['bronze']['duplicate_transaction_ids']}\")\n",
        "print(f\"  - Invalid Quantities: {quality_report['bronze']['invalid_quantities']}\")\n",
        "print(f\"  - Invalid Prices: {quality_report['bronze']['invalid_prices']}\")\n",
        "\n",
        "print(f\"\\nSilver Layer:\")\n",
        "print(f\"  - Records: {quality_report['silver']['record_count']:,}\")\n",
        "print(f\"  - Quality Passed: {quality_report['silver']['data_quality_passed']:,}\")\n",
        "print(f\"  - Avg Transaction Value: ${quality_report['silver']['avg_transaction_value']:.2f}\")\n",
        "print(f\"  - Total Revenue: ${quality_report['silver']['total_revenue']:,.2f}\")\n",
        "\n",
        "print(f\"\\nGold Layer:\")\n",
        "print(f\"  - Daily Summaries: {quality_report['gold']['daily_summaries']}\")\n",
        "print(f\"  - Date Range: {quality_report['gold']['date_range']['min_date']} to {quality_report['gold']['date_range']['max_date']}\")\n",
        "print(f\"  - Total Aggregated Revenue: ${quality_report['gold']['total_aggregated_revenue']:,.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance Optimization and Maintenance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance optimization\n",
        "print(\"Performing performance optimization...\")\n",
        "\n",
        "# Optimize all Delta tables\n",
        "tables_to_optimize = [\n",
        "    \"bronze_sales_transactions\",\n",
        "    \"silver_sales_transactions\",\n",
        "    \"gold_daily_sales_summary\",\n",
        "    \"gold_product_performance\",\n",
        "    \"gold_customer_segmentation\",\n",
        "    \"gold_regional_performance\"\n",
        "]\n",
        "\n",
        "for table in tables_to_optimize:\n",
        "    try:\n",
        "        print(f\"Optimizing table: {table}\")\n",
        "        spark.sql(f\"OPTIMIZE {table}\")\n",
        "        \n",
        "        # Update table statistics\n",
        "        spark.sql(f\"ANALYZE TABLE {table} COMPUTE STATISTICS\")\n",
        "        \n",
        "        print(f\"  âœ“ Optimized {table}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error optimizing {table}: {str(e)}\")\n",
        "\n",
        "print(\"Performance optimization completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup and maintenance\n",
        "print(\"Performing cleanup and maintenance...\")\n",
        "\n",
        "# Vacuum old files (removes files older than retention period)\n",
        "table_paths = [\n",
        "    bronze_table_path,\n",
        "    silver_table_path,\n",
        "    daily_sales_path,\n",
        "    product_performance_path,\n",
        "    customer_segmentation_path,\n",
        "    regional_performance_path\n",
        "]\n",
        "\n",
        "for path in table_paths:\n",
        "    try:\n",
        "        print(f\"Vacuuming: {path}\")\n",
        "        # Note: In production, be careful with vacuum - it permanently deletes files\n",
        "        # spark.sql(f\"VACUUM delta.`{path}` RETAIN 168 HOURS\")  # 7 days retention\n",
        "        print(f\"  âœ“ Would vacuum {path} (skipped for safety)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error vacuuming {path}: {str(e)}\")\n",
        "\n",
        "print(\"Cleanup and maintenance completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ETL Pipeline Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline execution summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ETL PIPELINE EXECUTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nâœ… COMPLETED SUCCESSFULLY:\")\n",
        "print(\"   â€¢ Bronze Layer: Raw data ingestion with metadata\")\n",
        "print(\"   â€¢ Silver Layer: Data cleansing and validation\")\n",
        "print(\"   â€¢ Gold Layer: Business-ready aggregations\")\n",
        "print(\"   â€¢ Data Quality: Comprehensive validation and monitoring\")\n",
        "print(\"   â€¢ Performance: Table optimization and statistics\")\n",
        "print(\"   â€¢ Maintenance: Cleanup and housekeeping\")\n",
        "\n",
        "print(\"\\nðŸ“Š DATA LAYERS CREATED:\")\n",
        "print(f\"   â€¢ Bronze: {bronze_table_path}\")\n",
        "print(f\"   â€¢ Silver: {silver_table_path}\")\n",
        "print(f\"   â€¢ Gold: {GOLD_PATH}/[multiple_aggregations]\")\n",
        "\n",
        "print(\"\\nðŸ” AVAILABLE TABLES:\")\n",
        "tables = spark.sql(\"SHOW TABLES\").collect()\n",
        "for table in tables:\n",
        "    if 'sales' in table.tableName or 'gold' in table.tableName or 'bronze' in table.tableName or 'silver' in table.tableName:\n",
        "        print(f\"   â€¢ {table.tableName}\")\n",
        "\n",
        "print(\"\\nðŸš€ NEXT STEPS:\")\n",
        "print(\"   1. Set up automated pipeline scheduling\")\n",
        "print(\"   2. Implement real-time streaming ingestion\")\n",
        "print(\"   3. Create Power BI reports and dashboards\")\n",
        "print(\"   4. Set up data quality alerts and monitoring\")\n",
        "print(\"   5. Implement machine learning models\")\n",
        "print(\"   6. Configure data governance and security\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ETL Pipeline completed successfully! ðŸŽ‰\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}