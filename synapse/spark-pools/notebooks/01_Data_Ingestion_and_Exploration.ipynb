{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Synapse Analytics Data Platform (ASADP)\n",
    "## Data Ingestion and Exploration Notebook\n",
    "\n",
    "This notebook demonstrates data ingestion patterns and exploratory data analysis using Azure Synapse Spark pools.\n",
    "\n",
    "### Objectives:\n",
    "1. Connect to various data sources\n",
    "2. Ingest data into the data lake\n",
    "3. Perform initial data exploration\n",
    "4. Implement data quality checks\n",
    "5. Create data profiling reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Lake Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Lake Storage configuration\n",
    "storage_account = \"your_storage_account_name\"\n",
    "container_raw = \"raw\"\n",
    "container_processed = \"processed\"\n",
    "container_curated = \"curated\"\n",
    "\n",
    "# Define data lake paths\n",
    "raw_path = f\"abfss://{container_raw}@{storage_account}.dfs.core.windows.net/\"\n",
    "processed_path = f\"abfss://{container_processed}@{storage_account}.dfs.core.windows.net/\"\n",
    "curated_path = f\"abfss://{container_curated}@{storage_account}.dfs.core.windows.net/\"\n",
    "\n",
    "print(f\"Raw data path: {raw_path}\")\n",
    "print(f\"Processed data path: {processed_path}\")\n",
    "print(f\"Curated data path: {curated_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_data(file_path, schema=None, header=True, delimiter=\",\"):\n",
    "    \"\"\"\n",
    "    Ingest CSV data with optional schema validation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if schema:\n",
    "            df = spark.read.option(\"header\", header).option(\"delimiter\", delimiter).schema(schema).csv(file_path)\n",
    "        else:\n",
    "            df = spark.read.option(\"header\", header).option(\"delimiter\", delimiter).option(\"inferSchema\", \"true\").csv(file_path)\n",
    "        \n",
    "        print(f\"Successfully ingested {df.count()} rows from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error ingesting data from {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def ingest_json_data(file_path, multiline=False):\n",
    "    \"\"\"\n",
    "    Ingest JSON data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.option(\"multiline\", multiline).json(file_path)\n",
    "        print(f\"Successfully ingested {df.count()} rows from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error ingesting JSON data from {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def ingest_parquet_data(file_path):\n",
    "    \"\"\"\n",
    "    Ingest Parquet data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.parquet(file_path)\n",
    "        print(f\"Successfully ingested {df.count()} rows from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error ingesting Parquet data from {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Data ingestion functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample sales data for demonstration\n",
    "def generate_sample_sales_data(num_records=10000):\n",
    "    \"\"\"\n",
    "    Generate sample sales data for demonstration purposes\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate data\n",
    "    data = {\n",
    "        'order_id': [f'ORD-{i:06d}' for i in range(1, num_records + 1)],\n",
    "        'customer_id': [f'CUST-{np.random.randint(1, 1000):04d}' for _ in range(num_records)],\n",
    "        'product_id': [f'PROD-{np.random.randint(1, 500):03d}' for _ in range(num_records)],\n",
    "        'order_date': pd.date_range(start='2023-01-01', end='2024-12-31', periods=num_records),\n",
    "        'quantity': np.random.randint(1, 10, num_records),\n",
    "        'unit_price': np.round(np.random.uniform(10, 1000, num_records), 2),\n",
    "        'discount': np.round(np.random.uniform(0, 0.3, num_records), 3),\n",
    "        'sales_channel': np.random.choice(['Online', 'Retail', 'Wholesale', 'Partner'], num_records),\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], num_records),\n",
    "        'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Sports', 'Books'], num_records)\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_pandas = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate derived fields\n",
    "    df_pandas['gross_amount'] = df_pandas['quantity'] * df_pandas['unit_price']\n",
    "    df_pandas['discount_amount'] = df_pandas['gross_amount'] * df_pandas['discount']\n",
    "    df_pandas['net_amount'] = df_pandas['gross_amount'] - df_pandas['discount_amount']\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    return df_spark\n",
    "\n",
    "# Generate sample data\n",
    "sales_df = generate_sample_sales_data(50000)\n",
    "print(f\"Generated sample sales data with {sales_df.count()} records\")\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, table_name=\"Unknown\"):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Data Quality Assessment for {table_name} ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_columns = len(df.columns)\n",
    "    \n",
    "    print(f\"Total Rows: {total_rows:,}\")\n",
    "    print(f\"Total Columns: {total_columns}\")\n",
    "    \n",
    "    # Check for null values\n",
    "    print(\"\\n--- Null Value Analysis ---\")\n",
    "    null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0]\n",
    "    \n",
    "    for column, null_count in null_counts.asDict().items():\n",
    "        null_percentage = (null_count / total_rows) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"{column}: {null_count:,} nulls ({null_percentage:.2f}%)\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(\"\\n--- Duplicate Analysis ---\")\n",
    "    distinct_rows = df.distinct().count()\n",
    "    duplicate_rows = total_rows - distinct_rows\n",
    "    duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "    \n",
    "    print(f\"Distinct Rows: {distinct_rows:,}\")\n",
    "    print(f\"Duplicate Rows: {duplicate_rows:,} ({duplicate_percentage:.2f}%)\")\n",
    "    \n",
    "    # Data type analysis\n",
    "    print(\"\\n--- Data Type Analysis ---\")\n",
    "    for field in df.schema.fields:\n",
    "        print(f\"{field.name}: {field.dataType}\")\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'total_columns': total_columns,\n",
    "        'null_counts': null_counts.asDict(),\n",
    "        'duplicate_rows': duplicate_rows,\n",
    "        'distinct_rows': distinct_rows\n",
    "    }\n",
    "\n",
    "# Assess data quality\n",
    "quality_report = assess_data_quality(sales_df, \"Sales Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=== Basic Statistics ===\")\n",
    "sales_df.describe().show()\n",
    "\n",
    "# Sales by channel\n",
    "print(\"\\n=== Sales by Channel ===\")\n",
    "channel_analysis = sales_df.groupBy(\"sales_channel\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"net_amount\").alias(\"total_sales\"),\n",
    "        avg(\"net_amount\").alias(\"avg_order_value\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_sales\"))\n",
    "\n",
    "channel_analysis.show()\n",
    "\n",
    "# Sales by region\n",
    "print(\"\\n=== Sales by Region ===\")\n",
    "region_analysis = sales_df.groupBy(\"region\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"net_amount\").alias(\"total_sales\"),\n",
    "        avg(\"net_amount\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_sales\"))\n",
    "\n",
    "region_analysis.show()\n",
    "\n",
    "# Sales by category\n",
    "print(\"\\n=== Sales by Category ===\")\n",
    "category_analysis = sales_df.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"net_amount\").alias(\"total_sales\"),\n",
    "        avg(\"net_amount\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_sales\"))\n",
    "\n",
    "category_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sales trend\n",
    "monthly_sales = sales_df \\\n",
    "    .withColumn(\"year_month\", date_format(\"order_date\", \"yyyy-MM\")) \\\n",
    "    .groupBy(\"year_month\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"net_amount\").alias(\"total_sales\"),\n",
    "        avg(\"net_amount\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"year_month\")\n",
    "\n",
    "print(\"=== Monthly Sales Trend ===\")\n",
    "monthly_sales.show(24)\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "monthly_sales_pd = monthly_sales.toPandas()\n",
    "monthly_sales_pd['year_month'] = pd.to_datetime(monthly_sales_pd['year_month'])\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Sales Analysis Dashboard', fontsize=16)\n",
    "\n",
    "# Monthly sales trend\n",
    "axes[0, 0].plot(monthly_sales_pd['year_month'], monthly_sales_pd['total_sales'], marker='o')\n",
    "axes[0, 0].set_title('Monthly Sales Trend')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Total Sales')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sales by channel\n",
    "channel_data = channel_analysis.toPandas()\n",
    "axes[0, 1].bar(channel_data['sales_channel'], channel_data['total_sales'])\n",
    "axes[0, 1].set_title('Sales by Channel')\n",
    "axes[0, 1].set_xlabel('Sales Channel')\n",
    "axes[0, 1].set_ylabel('Total Sales')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sales by region\n",
    "region_data = region_analysis.toPandas()\n",
    "axes[1, 0].pie(region_data['total_sales'], labels=region_data['region'], autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Sales Distribution by Region')\n",
    "\n",
    "# Sales by category\n",
    "category_data = category_analysis.toPandas()\n",
    "axes[1, 1].barh(category_data['category'], category_data['total_sales'])\n",
    "axes[1, 1].set_title('Sales by Category')\n",
    "axes[1, 1].set_xlabel('Total Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer segmentation analysis\n",
    "customer_metrics = sales_df.groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_frequency\"),\n",
    "        sum(\"net_amount\").alias(\"total_spent\"),\n",
    "        avg(\"net_amount\").alias(\"avg_order_value\"),\n",
    "        max(\"order_date\").alias(\"last_order_date\"),\n",
    "        min(\"order_date\").alias(\"first_order_date\")\n",
    "    )\n",
    "\n",
    "# Add recency calculation\n",
    "current_date = lit(datetime.now().date())\n",
    "customer_metrics = customer_metrics.withColumn(\n",
    "    \"recency_days\", \n",
    "    datediff(current_date, col(\"last_order_date\"))\n",
    ")\n",
    "\n",
    "print(\"=== Customer Metrics Summary ===\")\n",
    "customer_metrics.describe().show()\n",
    "\n",
    "# Product performance analysis\n",
    "product_performance = sales_df.groupBy(\"product_id\", \"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"net_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"unit_price\").alias(\"avg_price\"),\n",
    "        avg(\"discount\").alias(\"avg_discount\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(\"\\n=== Top 20 Products by Revenue ===\")\n",
    "product_performance.show(20)\n",
    "\n",
    "# Seasonal analysis\n",
    "seasonal_analysis = sales_df \\\n",
    "    .withColumn(\"quarter\", quarter(\"order_date\")) \\\n",
    "    .withColumn(\"month\", month(\"order_date\")) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"order_date\")) \\\n",
    "    .groupBy(\"quarter\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"net_amount\").alias(\"total_sales\"),\n",
    "        avg(\"net_amount\").alias(\"avg_order_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"quarter\")\n",
    "\n",
    "print(\"\\n=== Quarterly Sales Analysis ===\")\n",
    "seasonal_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_profile(df, table_name):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data profiling report\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        'table_name': table_name,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'row_count': df.count(),\n",
    "        'column_count': len(df.columns),\n",
    "        'columns': {}\n",
    "    }\n",
    "    \n",
    "    for column in df.columns:\n",
    "        col_stats = df.select(column).describe().collect()\n",
    "        \n",
    "        column_profile = {\n",
    "            'data_type': str(df.schema[column].dataType),\n",
    "            'null_count': df.filter(col(column).isNull()).count(),\n",
    "            'distinct_count': df.select(column).distinct().count(),\n",
    "            'statistics': {row['summary']: row[column] for row in col_stats if row[column] is not None}\n",
    "        }\n",
    "        \n",
    "        # Calculate null percentage\n",
    "        column_profile['null_percentage'] = (column_profile['null_count'] / profile['row_count']) * 100\n",
    "        \n",
    "        # Calculate uniqueness percentage\n",
    "        column_profile['uniqueness_percentage'] = (column_profile['distinct_count'] / profile['row_count']) * 100\n",
    "        \n",
    "        profile['columns'][column] = column_profile\n",
    "    \n",
    "    return profile\n",
    "\n",
    "# Generate profile\n",
    "data_profile = generate_data_profile(sales_df, \"Sales Data\")\n",
    "\n",
    "print(\"=== Data Profiling Report ===\")\n",
    "print(f\"Table: {data_profile['table_name']}\")\n",
    "print(f\"Generated: {data_profile['timestamp']}\")\n",
    "print(f\"Rows: {data_profile['row_count']:,}\")\n",
    "print(f\"Columns: {data_profile['column_count']}\")\n",
    "\n",
    "print(\"\\n--- Column Profiles ---\")\n",
    "for col_name, col_profile in data_profile['columns'].items():\n",
    "    print(f\"\\n{col_name}:\")\n",
    "    print(f\"  Data Type: {col_profile['data_type']}\")\n",
    "    print(f\"  Null Count: {col_profile['null_count']} ({col_profile['null_percentage']:.2f}%)\")\n",
    "    print(f\"  Distinct Values: {col_profile['distinct_count']} ({col_profile['uniqueness_percentage']:.2f}%)\")\n",
    "    \n",
    "    if col_profile['statistics']:\n",
    "        print(f\"  Statistics: {col_profile['statistics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to different layers of the data lake\n",
    "\n",
    "# Save raw data (CSV format)\n",
    "raw_sales_path = f\"{raw_path}sales/year={datetime.now().year}/month={datetime.now().month:02d}/\"\n",
    "print(f\"Saving raw data to: {raw_sales_path}\")\n",
    "\n",
    "sales_df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(raw_sales_path)\n",
    "\n",
    "# Save processed data (Parquet format with partitioning)\n",
    "processed_sales_path = f\"{processed_path}sales/\"\n",
    "print(f\"Saving processed data to: {processed_sales_path}\")\n",
    "\n",
    "sales_df.withColumn(\"year\", year(\"order_date\")) \\\n",
    "    .withColumn(\"month\", month(\"order_date\")) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(processed_sales_path)\n",
    "\n",
    "# Save aggregated data (Delta format)\n",
    "curated_sales_path = f\"{curated_path}sales_summary/\"\n",
    "print(f\"Saving curated data to: {curated_sales_path}\")\n",
    "\n",
    "# Create daily summary\n",
    "daily_summary = sales_df \\\n",
    "    .withColumn(\"order_date_only\", to_date(\"order_date\")) \\\n",
    "    .groupBy(\"order_date_only\", \"sales_channel\", \"region\", \"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"gross_amount\").alias(\"gross_sales\"),\n",
    "        sum(\"discount_amount\").alias(\"total_discounts\"),\n",
    "        sum(\"net_amount\").alias(\"net_sales\"),\n",
    "        avg(\"net_amount\").alias(\"avg_order_value\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        countDistinct(\"product_id\").alias(\"unique_products\")\n",
    "    )\n",
    "\n",
    "daily_summary.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .save(curated_sales_path)\n",
    "\n",
    "print(\"\\nData saved successfully to all layers!\")\n",
    "print(f\"Raw layer: {raw_sales_path}\")\n",
    "print(f\"Processed layer: {processed_sales_path}\")\n",
    "print(f\"Curated layer: {curated_sales_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Data Ingestion and Exploration Summary ===\")\n",
    "print(f\"âœ… Successfully processed {sales_df.count():,} sales records\")\n",
    "print(f\"âœ… Performed comprehensive data quality assessment\")\n",
    "print(f\"âœ… Generated detailed data profiling report\")\n",
    "print(f\"âœ… Conducted exploratory data analysis\")\n",
    "print(f\"âœ… Created visualizations and insights\")\n",
    "print(f\"âœ… Saved data to medallion architecture layers\")\n",
    "\n",
    "print(\"\\n=== Key Insights ===\")\n",
    "print(f\"â€¢ Total sales volume: ${sales_df.agg(sum('net_amount')).collect()[0][0]:,.2f}\")\n",
    "print(f\"â€¢ Average order value: ${sales_df.agg(avg('net_amount')).collect()[0][0]:.2f}\")\n",
    "print(f\"â€¢ Unique customers: {sales_df.select('customer_id').distinct().count():,}\")\n",
    "print(f\"â€¢ Unique products: {sales_df.select('product_id').distinct().count():,}\")\n",
    "\n",
    "print(\"\\n=== Next Steps ===\")\n",
    "print(\"1. Implement automated data quality monitoring\")\n",
    "print(\"2. Set up data lineage tracking\")\n",
    "print(\"3. Create real-time data ingestion pipelines\")\n",
    "print(\"4. Develop machine learning models for predictive analytics\")\n",
    "print(\"5. Build interactive dashboards and reports\")\n",
    "print(\"6. Implement data governance and security policies\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Data ingestion and exploration completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}